import os
import streamlit as st 
from dotenv import load_dotenv
from operator import itemgetter
from pathlib import Path
import re 
from langchain_community.document_loaders import TextLoader 
from langchain_community.vectorstores import Chroma 
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_core.documents import Document 
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("ERROR: GEMINI API KEY not found. Please set it in Streamlit Secrets.")
    st.stop()

# ==================================================================
# 0. AYARLAR
# ==================================================================
DATA_PATH = "data/words.txt" # Ã–rnek dosya yolu
CHROMA_DB_DIR = "chroma_db/chroma_db_streamlit" # Streamlit iÃ§in ayrÄ± bir DB klasÃ¶rÃ¼
EMBEDDING_MODEL = "all-MiniLM-L6-v2" 
GENERATION_MODEL = "gemini-2.5-flash" 

# API anahtarÄ±nÄ± Ã§ek
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    # Streamlit'te hata gÃ¶sterme
    st.error("ERROR: Failed to load GEMINI API KEY. Check your .env file or Streamlit secrets.")
    st.stop()

# Veri dosyasÄ± kontrolÃ¼
if not Path(DATA_PATH).exists():
    st.error(f"ERROR: Data file not found: {DATA_PATH}. Please create file 'data/words.txt'.")
    st.stop()

# Sistem Prompt'u (LLM'e rolÃ¼nÃ¼ ve gÃ¶revinin sÄ±nÄ±rlarÄ±nÄ± tanÄ±mlar)
RAG_SYSTEM_PROMPT = """
Sen, sana verilen verisetindeki kelimeleri analiz eden, Ä°ngilizce kelimelerin TÃ¼rkÃ§e anlamlarÄ±nÄ± ve TÃ¼rkÃ§e kelimelerin Ä°ngilizce anlamlarÄ±nÄ± ve Ã¶rnek cÃ¼mlelerini saÄŸlayan bir Dil eÄŸitmenisin.

***GÃ–REV KURALLARI***

1.  **Sorgu Tipi Belirleme:** KullanÄ±cÄ±nÄ±n sorusunun amacÄ±na gÃ¶re hareket et:
    **Tekil Kelime Sorgusu (Ã–rn: "jacket kelimesi ne demek?"):** Sadece kullanÄ±cÄ±nÄ±n sorduÄŸu Ä°ngilizce kelimeyi BaÄŸlam'da ara. Bulursan, BaÄŸlam'daki seviye bilgisini (A1, B2 vb.) kullanarak formatÄ± *sadece o kelime* iÃ§in uygula ve o kelimeye ait Ä°ngilizce Ã¶rnek cÃ¼mleler Ã¼ret.
    **Seviye/Toplu Sorgu (Ã–rn: "A1 kelimelerini ver."):** BaÄŸlam'dan istenen seviyeye uygun (varsa) **minimum 3 kelime** seÃ§ ve formatÄ± uygula.

2.  **CÃ¼mle KuralÄ±:** SeÃ§tiÄŸin her Ä°ngilizce kelime iÃ§in, kelimenin farklÄ± kullanÄ±m tonlarÄ±nÄ± gÃ¶steren **baÄŸlamdan baÄŸÄ±msÄ±z en az 3 farklÄ± doÄŸru Ä°ngilizce Ã¶rnek cÃ¼mle** kur.
3.  **Format KuralÄ±:** CevabÄ±nÄ± sadece aÅŸaÄŸÄ±daki **Ã–rnek Cevap FormatÄ±na** uygun olarak dÃ¼zenle.

---
BaÄŸlam (Context):
{context}
---
Soru (Question): {input}

***
Ã–rnek Cevap FormatÄ±:

### 1. [Ä°ngilizce Kelime]
- **TÃ¼rkÃ§e AnlamÄ±:** [Anlam]
- **Ã–rnek CÃ¼mleler:**
    1. [CÃ¼mle 1]
    2. [CÃ¼mle 2]
    3. [CÃ¼mle 3]
### 2. [TÃ¼rkÃ§e Kelime]
- **Ä°ngilizce AnlamÄ±:** [Anlam]
- **Ã–rnek CÃ¼mleler:**
    1. [CÃ¼mle 1]
    2. [CÃ¼mle 2]
    2. [CÃ¼mle 3]
***
"""

# ==================================================================
# 1. RAG FONKSÄ°YONLARI (Streamlit Ã–nbellekleme ile)
# ==================================================================

@st.cache_resource
def index_data(data_file_path: str, db_dir: str):
    
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    # VektÃ¶r VeritabanÄ± (ChromaDB) KalÄ±cÄ±lÄ±k KontrolÃ¼
    if Path(db_dir).exists():
        try:
            vectorstore = Chroma(
                persist_directory=db_dir, 
                embedding_function=embeddings
            )
            # EÄŸer kayÄ±t varsa, direkt yÃ¼kle ve bitir.
            if vectorstore._collection.count() > 0:
                st.info(f"Vector database loaded from disk. Total records: {vectorstore._collection.count()}")
                return vectorstore
        except Exception as e:
            # Hata oluÅŸursa (bozuk dosya vs.), yeniden oluÅŸturmaya geÃ§
            st.warning(f"Error loading ChromaDB ({e}). Rebuilding...")
    
    # EÄŸer diskte yoksa veya bozuksa: Veri YÃ¼kleme ve BÃ¶lme
    with st.spinner("The data is being loaded, split by line, and vectorized... This may take some time."):
        
        # Veri YÃ¼kleyici
        loader = TextLoader(data_file_path, encoding='utf-8')
        raw_documents = loader.load()

        # SatÄ±r BazlÄ± BÃ¶lme (Chunking)
        splits = []
        if raw_documents and raw_documents[0].page_content:
            # Her satÄ±rÄ± (kelime kaydÄ±nÄ±) ayrÄ± bir Document olarak ele al
            for i, line in enumerate(raw_documents[0].page_content.splitlines()):
                cleaned_line = line.strip()
                if cleaned_line:
                    splits.append(
                        Document(
                            page_content=cleaned_line,
                            metadata={"source": Path(data_file_path).name, "line": i + 1}
                        )
                    )
        
        if not splits:
            raise ValueError(f"'{data_file_path}' file is empty or unreadable.")
            
        st.write(f"-> {len(raw_documents)} document, {len(splits)} divided into piece.")
        
        # VektÃ¶r VeritabanÄ± OluÅŸturma ve Kaydetme
        st.warning("VektÃ¶r database is creating...")
        vectorstore = Chroma.from_documents(
            documents=splits, 
            embedding=embeddings,
            persist_directory=db_dir 
        )
        vectorstore.persist() 
        st.success(f"Vector database created and saved in {db_dir} folder. Total records: {vectorstore._collection.count()}")
        
    return vectorstore

@st.cache_resource
def create_rag_chain(_vectorstore: Chroma):
    with st.spinner(f"RAG Chain ({GENERATION_MODEL}) is being established..."):

        retriever = _vectorstore.as_retriever(search_kwargs={"k": 7}) 

        
        llm = ChatGoogleGenerativeAI(
            model=GENERATION_MODEL, 
            temperature=0.7,
            google_api_key=GEMINI_API_KEY
        )
        prompt = ChatPromptTemplate.from_messages([
            ("system", RAG_SYSTEM_PROMPT),
            ("human", "{input}")
        ])
        
        # LCEL RAG Zinciri OluÅŸturma
        rag_chain = (
            RunnablePassthrough.assign(
                context=itemgetter("input") | retriever
            )
            | prompt
            | llm
            | StrOutputParser()
        )
    st.success("RAG Chain is installed and ready to use.")
    return rag_chain

# ==================================================================
# 3. STREAMLIT ARAYÃœZÃœ
# ==================================================================

def main():
    st.set_page_config(page_title="Translation Bot", layout="wide")
    st.title("ğŸ“š RAG-Powered Translation Bot")
    st.caption("LangChain (LCEL) + Gemini Flash + ChromaDB")

    # --- RAG Sistemini Kurma ---
    try:
        # ChromaDB'yi diskten yÃ¼kle veya oluÅŸtur
        _vectorstore = index_data(DATA_PATH, CHROMA_DB_DIR) 
        
        rag_chain = create_rag_chain(_vectorstore)
    except Exception as e:
        st.error(f"A critical error occurred during system installation: {e}")
        return 

    # --- Sohbet GeÃ§miÅŸi YÃ¶netimi ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Sohbet GeÃ§miÅŸini GÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- KullanÄ±cÄ± GiriÅŸi ve YanÄ±t Ãœretme ---
    if prompt := st.chat_input("Ask me a word..."):
        # KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Asistan yanÄ±tÄ±nÄ± Ã¼ret ve gÃ¶ster
        with st.chat_message("assistant"):
            with st.spinner("Searching for and creating answers..."):
                try:
                    input_data = {"input": prompt}
                    result = rag_chain.invoke(input_data)
                    
                    st.markdown(result)
                    
                    # CevabÄ± geÃ§miÅŸe kaydet
                    st.session_state.messages.append({"role": "assistant", "content": result})
                    
                except Exception as e:
                    error_msg = f"An error occurred while generating the answer: {e}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()

